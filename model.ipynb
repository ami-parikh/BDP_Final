{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Troll Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark\n",
    "# findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.eventLog.enabled', 'true'),\n",
       " ('spark.yarn.appMasterEnv.MKL_NUM_THREADS', '1'),\n",
       " ('spark.driver.port', '41011'),\n",
       " ('spark.executor.instances', '32'),\n",
       " ('spark.driver.memory', '4g'),\n",
       " ('spark.sql.queryExecutionListeners',\n",
       "  'com.cloudera.spark.lineage.NavigatorQueryListener'),\n",
       " ('spark.ui.killEnabled', 'true'),\n",
       " ('spark.lineage.log.dir', '/var/log/spark/lineage'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  'md01.rcc.local,md02.rcc.local'),\n",
       " ('spark.app.id', 'application_1547750003855_6781'),\n",
       " ('spark.yarn.am.extraLibraryPath',\n",
       "  '/opt/cloudera/parcels/CDH-6.1.0-1.cdh6.1.0.p0.770702/lib/hadoop/lib/native'),\n",
       " ('spark.serializer', 'org.apache.spark.serializer.KryoSerializer'),\n",
       " ('spark.executor.extraLibraryPath',\n",
       "  '/opt/cloudera/parcels/CDH-6.1.0-1.cdh6.1.0.p0.770702/lib/hadoop/lib/native'),\n",
       " ('spark.driver.extraLibraryPath',\n",
       "  '/opt/cloudera/parcels/CDH-6.1.0-1.cdh6.1.0.p0.770702/lib/hadoop/lib/native'),\n",
       " ('spark.eventLog.dir', 'hdfs://nameservice1/user/spark/applicationHistory'),\n",
       " ('spark.dynamicAllocation.executorIdleTimeout', '60'),\n",
       " ('spark.cores.max', '4'),\n",
       " ('spark.io.encryption.enabled', 'false'),\n",
       " ('spark.yarn.historyServer.address', 'http://hd01.rcc.local:18088'),\n",
       " ('spark.executor.cores', '4'),\n",
       " ('spark.authenticate', 'false'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '/opt/cloudera/parcels/CDH/lib/spark/python/lib/py4j-0.10.7-src.zip:/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip<CPS>/opt/cloudera/parcels/CDH-6.1.0-1.cdh6.1.0.p0.770702/lib/spark/python/lib/py4j-0.10.7-src.zip<CPS>/opt/cloudera/parcels/CDH-6.1.0-1.cdh6.1.0.p0.770702/lib/spark/python/lib/pyspark.zip'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1547750003855_6781'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.executor.memory', '8g'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.yarn.jars',\n",
       "  'local:/opt/cloudera/parcels/CDH-6.1.0-1.cdh6.1.0.p0.770702/lib/spark/jars/*,local:/opt/cloudera/parcels/CDH-6.1.0-1.cdh6.1.0.p0.770702/lib/spark/hive/*'),\n",
       " ('spark.network.crypto.enabled', 'false'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.RM_HA_URLS',\n",
       "  'md01.rcc.local:8088,md02.rcc.local:8088'),\n",
       " ('spark.shuffle.service.enabled', 'true'),\n",
       " ('spark.yarn.historyServer.allowTracking', 'true'),\n",
       " ('spark.executorEnv.MKL_NUM_THREADS', '1'),\n",
       " ('spark.driver.appUIAddress', 'http://md01.rcc.local:4052'),\n",
       " ('spark.ui.enabled', 'true'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://md01.rcc.local:8088/proxy/application_1547750003855_6781,http://md02.rcc.local:8088/proxy/application_1547750003855_6781'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.dynamicAllocation.schedulerBacklogTimeout', '1'),\n",
       " ('spark.yarn.appMasterEnv.OPENBLAS_NUM_THREADS', '1'),\n",
       " ('spark.shuffle.service.port', '7337'),\n",
       " ('spark.lineage.enabled', 'true'),\n",
       " ('spark.extraListeners', 'com.cloudera.spark.lineage.NavigatorAppListener'),\n",
       " ('spark.yarn.config.gatewayPath', '/opt/cloudera/parcels'),\n",
       " ('spark.port.maxRetries', '60'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.driver.host', 'md01.rcc.local'),\n",
       " ('spark.sql.warehouse.dir', '/user/hive/warehouse'),\n",
       " ('spark.app.name', 'Spark Updated Conf'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.dynamicAllocation.minExecutors', '0'),\n",
       " ('spark.yarn.config.replacementPath', '{{HADOOP_COMMON_HOME}}/../../..'),\n",
       " ('spark.dynamicAllocation.enabled', 'true'),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.executorEnv.OPENBLAS_NUM_THREADS', '1'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('rus_trolls').getOrCreate()\n",
    "\n",
    "# Change configuration settings on Spark \n",
    "conf = spark.sparkContext._conf.setAll([('spark.executor.memory', '8g'), ('spark.app.name', 'Spark Updated Conf'), ('spark.executor.cores', '4'), ('spark.cores.max', '4'), ('spark.driver.memory','4g')])\n",
    "\n",
    "# Print spark configuration settings\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "# hc = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|   class|          tweet_text|\n",
      "+--------+--------------------+\n",
      "|innocent|@angelayee hey gi...|\n",
      "|innocent|i HATE the rain i...|\n",
      "|innocent|@jasontryfon Thin...|\n",
      "|innocent|o gosh need help ...|\n",
      "|innocent|Aww man, it sound...|\n",
      "|innocent|watching the dark...|\n",
      "|innocent|just learned I ca...|\n",
      "|innocent|Uuuuuuuuuuuugh an...|\n",
      "|innocent|@jacobawhite Have...|\n",
      "|innocent|@tkorte20 peeps I...|\n",
      "+--------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hive_context = HiveContext(sc)\n",
    "df = hive_context.table(\"tywang.trolls_non_trolls\")\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read.csv('data/trolls_non_trolls.csv', inferSchema=True, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|   class|          tweet_text|\n",
      "+--------+--------------------+\n",
      "|innocent|@angelayee hey gi...|\n",
      "|innocent|i HATE the rain i...|\n",
      "|innocent|@jasontryfon Thin...|\n",
      "|innocent|o gosh need help ...|\n",
      "|innocent|Aww man, it sound...|\n",
      "|innocent|watching the dark...|\n",
      "|innocent|just learned I ca...|\n",
      "|innocent|Uuuuuuuuuuuugh an...|\n",
      "|innocent|@jacobawhite Have...|\n",
      "|innocent|@tkorte20 peeps I...|\n",
      "|innocent|Doubled up then l...|\n",
      "|innocent|ouch, dont ya jus...|\n",
      "|innocent|@gretskiarienne j...|\n",
      "|innocent|@catiepoo123 seem...|\n",
      "|innocent|   n im still @ work|\n",
      "|innocent|Bad TwitThis!  Th...|\n",
      "|innocent|At Hooter's for a...|\n",
      "|innocent|Watching @Hepnova...|\n",
      "|innocent|ng. Very unfair, ...|\n",
      "|innocent|I really wish I c...|\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldColumns = df.schema.names\n",
    "newColumns = [\"label\", \"tweet_text\"]\n",
    "\n",
    "for i in range(0, len(oldColumns)):\n",
    "    df = df.withColumnRenamed(oldColumns[i], newColumns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|   label|  count|\n",
      "+--------+-------+\n",
      "|   troll|1358472|\n",
      "|innocent|1600000|\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|   label|  count|\n",
      "+--------+-------+\n",
      "|   troll|1286290|\n",
      "|innocent|1600000|\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropna().groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_troll = df.select(\"class\").rdd.flatMap(lambda x: 1 if x =='troll' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def modify_values(r):\n",
    "    if r == \"troll\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "ol_val = udf(modify_values, IntegerType())\n",
    "df = df.withColumn(\"label\", ol_val(df[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|label|  count|\n",
      "+-----+-------+\n",
      "|    1|1286290|\n",
      "|    0|1600000|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropna().groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "def lower_clean_str(x):\n",
    "    punc='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    lowercased_str = x.lower()\n",
    "    for ch in punc:\n",
    "        lowercased_str = lowercased_str.replace(ch, '')\n",
    "    return lowercased_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = udf(lower_clean_str, StringType())\n",
    "df = df.withColumn(\"tweet_text\", clean_text(df[\"tweet_text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|          tweet_text|\n",
      "+-----+--------------------+\n",
      "|    0|angelayee hey gir...|\n",
      "|    0|i hate the rain i...|\n",
      "|    0|jasontryfon thing...|\n",
      "|    0|o gosh need help ...|\n",
      "|    0|aww man it sounds...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|tweet_text                                                                                                                        |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|angelayee hey girlie if youre talking about go hard or go home on sipping your cocktailsgo home lol you gotta work in the morning |\n",
      "|i hate the rain i hate the rain i hate the rain                                                                                   |\n",
      "|jasontryfon things are great here thanks  was kind of looking forward to a tstormbut it didnt happen   oh well                    |\n",
      "|o gosh need help uploadin pic                                                                                                     |\n",
      "|aww man it sounds like everyone has ice cream but me                                                                              |\n",
      "|watching the dark knight with gram i miss my accountant babe                                                                      |\n",
      "|just learned i cant clap amp sing at the same time hence i suck  ignitephx                                                        |\n",
      "|uuuuuuuuuuuugh another night sleeping alone                                                                                       |\n",
      "|jacobawhite have you stopped twitting                                                                                             |\n",
      "|tkorte20 peeps i know youre upset the magic didnt win  you and me both                                                            |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('tweet_text').show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|          tweet_text|\n",
      "+-----+--------------------+\n",
      "|    0|\t we love album  ...|\n",
      "+-----+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and test\n",
    "splits = df.randomSplit([0.8, 0.2])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]\n",
    "\n",
    "train_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|               words|\n",
      "+-----+--------------------+\n",
      "|    0|[angelayee, hey, ...|\n",
      "|    0|[i, hate, the, ra...|\n",
      "|    0|[jasontryfon, thi...|\n",
      "|    0|[o, gosh, need, h...|\n",
      "|    0|[aww, man, it, so...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"tweet_text\", outputCol=\"words\")\n",
    "df_words = tokenizer.transform(df)\n",
    "df_words = df_words.drop(\"tweet_text\")\n",
    "df_words.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|class|    filtered_content|\n",
      "+-----+--------------------+\n",
      "|    0|[angelayee, hey, ...|\n",
      "|    0|[hate, rain, hate...|\n",
      "|    0|[jasontryfon, thi...|\n",
      "|    0|[o, gosh, need, h...|\n",
      "|    0|[aww, man, sounds...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_content\")\n",
    "df_words_removed_sw = remover.transform(df_words)\n",
    "\n",
    "# Drop the redundant source column\n",
    "df_words_removed_sw = df_words_removed_sw.drop(\"words\")\n",
    "df_words_removed_sw.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|class|            features|\n",
      "+-----+--------------------+\n",
      "|    0|(20,[3,4,5,6,7,9,...|\n",
      "|    0|(20,[9,18],[3.552...|\n",
      "|    0|(20,[0,3,4,6,9,10...|\n",
      "|    0|(20,[0,7,10,14,15...|\n",
      "|    0|(20,[6,7,10,13,19...|\n",
      "|    0|(20,[3,8,9,11,15,...|\n",
      "|    0|(20,[1,2,5,6,9,11...|\n",
      "|    0|(20,[4,7,9,16,19]...|\n",
      "|    0|(20,[3,15,17],[1....|\n",
      "|    0|(20,[6,12,13,16,1...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashingTF = HashingTF(inputCol=\"filtered_content\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(df_words_removed_sw)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "nlpdf = idfModel.transform(featurizedData)\n",
    "nlpdf.select(\"label\", \"features\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Learn a mapping from words to Vectors\n",
    "# from pyspark.ml.feature import Word2Vec\n",
    "# word2Vec = Word2Vec(vectorSize=5, minCount=0, inputCol=\"filtered_content\", outputCol=\"wordVectors\")\n",
    "# w2VM = word2Vec.fit(df_words_removed_sw)\n",
    "# nlpdf_w2v = w2VM.transform(df_words_removed_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+-----------+----------+\n",
      "|class|filtered_content|rawFeatures|  features|\n",
      "+-----+----------------+-----------+----------+\n",
      "|    0|              []| (20,[],[])|(20,[],[])|\n",
      "+-----+----------------+-----------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and test\n",
    "splits = nlpdf.randomSplit([0.8, 0.2])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]\n",
    "\n",
    "train_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for Logistic Regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lgr = LogisticRegression(maxIter=10, featuresCol=\"features\", labelCol=\"label\", regParam=0.01)\n",
    "\n",
    "# Fit the model to the data.\n",
    "lgrm = lgr.fit(train_df)\n",
    "\n",
    "# Given a dataset, predict each point's label, and show the results.\n",
    "predictions = lgrm.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6730933633295838\n",
      "0.6675709023001818\n"
     ]
    }
   ],
   "source": [
    "# print evaluation metrics\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"}))\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, featuresCol=\"features\", labelCol=\"label\", regParam=0.01)\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, lr])\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(train_df)\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "predictions = cvModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8516960594919092\n",
      "0.8501931325857981\n"
     ]
    }
   ],
   "source": [
    "#print evaluation metrics\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"}))\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tokenizer_e452ecdc0d9a,\n",
       " StopWordsRemover_2c2e855698f6,\n",
       " HashingTF_c2f71bd19c7d,\n",
       " IDF_30e7cf4267b7,\n",
       " LogisticRegressionModel: uid = LogisticRegression_2e1ca998c88e, numClasses = 2, numFeatures = 1000]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel.bestModel.save('best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis.ipynb\tdata_cleaning1.ipynb  data_cleaning.ipynb  model.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+-----------+----------+\n",
      "|class|filtered_content|rawFeatures|  features|\n",
      "+-----+----------------+-----------+----------+\n",
      "|    0|              []| (20,[],[])|(20,[],[])|\n",
      "|    0|              []| (20,[],[])|(20,[],[])|\n",
      "|    0|              []| (20,[],[])|(20,[],[])|\n",
      "|    0|              []| (20,[],[])|(20,[],[])|\n",
      "|    0|              []| (20,[],[])|(20,[],[])|\n",
      "+-----+----------------+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.plot(cvModel.summary.roc.select('FPR').collect(),\n",
    "         cvModel.summary.roc.select('TPR').collect())\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark 4G 32e",
   "language": "python",
   "name": "pyspark2_4g32e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
